{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307d3208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "\n",
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "\n",
    "def is_cuda():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
    "\n",
    "\n",
    "def supports_host_descriptor():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9\n",
    "\n",
    "\n",
    "def is_blackwell():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] == 10\n",
    "\n",
    "\n",
    "def is_hopper():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] == 9\n",
    "\n",
    "if is_hip():\n",
    "    NUM_STAGES_OPTIONS = [1]\n",
    "elif supports_host_descriptor():\n",
    "    NUM_STAGES_OPTIONS = [2, 3, 4]\n",
    "else:\n",
    "    NUM_STAGES_OPTIONS = [2, 3, 4]\n",
    "\n",
    "def _host_descriptor_pre_hook(nargs):\n",
    "    BLOCK_M = nargs[\"BLOCK_M\"]\n",
    "    BLOCK_N = nargs[\"BLOCK_N\"]\n",
    "    HEAD_DIM = nargs[\"HEAD_DIM\"]\n",
    "    if not isinstance(nargs[\"desc_q\"], tl.tensor_descriptor):\n",
    "        return\n",
    "    nargs[\"desc_q\"].block_shape = [BLOCK_M, HEAD_DIM]\n",
    "    if nargs[\"FP8_OUTPUT\"]:\n",
    "        nargs[\"desc_v\"].block_shape = [HEAD_DIM, BLOCK_N]\n",
    "    else:\n",
    "        nargs[\"desc_v\"].block_shape = [BLOCK_N, HEAD_DIM]\n",
    "    nargs[\"desc_k\"].block_shape = [BLOCK_N, HEAD_DIM]\n",
    "    nargs[\"desc_o\"].block_shape = [BLOCK_M, HEAD_DIM]\n",
    "    \n",
    "configs = [\n",
    "    triton.Config({'BLOCK_M': BM, 'BLOCK_N': BN}, num_stages=s, num_warps=w, pre_hook=_host_descriptor_pre_hook) \\\n",
    "    for BM in [64, 128]\\\n",
    "    for BN in [32, 64, 128]\\\n",
    "    for s in NUM_STAGES_OPTIONS \\\n",
    "    for w in [4, 8]\\\n",
    "]\n",
    "\n",
    "def keep(conf):\n",
    "    BLOCK_M = conf.kwargs[\"BLOCK_M\"]\n",
    "    BLOCK_N = conf.kwargs[\"BLOCK_N\"]\n",
    "    return not (is_cuda() and torch.cuda.get_device_capability()[0] == 9 and BLOCK_M * BLOCK_N < 128 * 128\n",
    "                and conf.num_warps == 8)\n",
    "    \n",
    "def prune_invalid_configs(configs, named_args, **kwargs):\n",
    "    S = kwargs[\"S\"]\n",
    "\n",
    "    # Filter out configs where BLOCK_M > N_CTX\n",
    "    return [conf for conf in configs if conf.kwargs.get(\"BLOCK_M\", 0) <= S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _maybe_make_tensor_desc(desc_or_ptr, shape, strides, block_shape):\n",
    "    if isinstance(desc_or_ptr, tl.tensor_descriptor):\n",
    "        return desc_or_ptr\n",
    "    else:\n",
    "        return tl.make_tensor_descriptor(desc_or_ptr, shape, strides, block_shape)\n",
    "\n",
    "\n",
    "@triton.autotune(configs=list(filter(keep, configs)), key=[\"S\", \"HEAD_DIM\", \"FP8_OUTPUT\", \"warp_specialize\"],\n",
    "                 prune_configs_by={'early_config_prune': prune_invalid_configs})\n",
    "@triton.jit\n",
    "def _attn_fwd_kernel(\n",
    "    sm_scale,\n",
    "    M,\n",
    "    Bs,\n",
    "    NH,\n",
    "    q_desc,\n",
    "    k_desc,\n",
    "    v_desc,\n",
    "    o_desc,\n",
    "    S,\n",
    "    HEAD_DIM:tl.constexpr,\n",
    "    BLOCK_M:tl.constexpr,\n",
    "    BLOCK_N:tl.constexpr,\n",
    "    FP8_OUTPUT:tl.constexpr,\n",
    "    STAGE:tl.constexpr,\n",
    "    warp_specialize:tl.constexpr,\n",
    "    IS_HOPPER:tl.constexpr,\n",
    "):\n",
    "    dtype = tl.float8e5 if FP8_OUTPUT else tl.float16\n",
    "    tl.static_assert(BLOCK_N <= HEAD_DIM)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _attention(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, causal: bool, sm_scale: float, warp_specialize: bool = True):\n",
    "        \"\"\"\n",
    "        q.shape: (Bs, NHead, SeqLen, HeadDim)\n",
    "        \"\"\"\n",
    "        assert q.dim() == 4\n",
    "        Bs, NH, S, HEAD_DIM = q.shape\n",
    "        HEAD_DIM_K, HEAD_DIM_V = k.shape[-1], v.shape[-1]\n",
    "        assert HEAD_DIM == HEAD_DIM_K == HEAD_DIM_V\n",
    "        assert HEAD_DIM_K in {16, 32, 64, 128, 256}\n",
    "\n",
    "        o = torch.empty_like(q)\n",
    "        stage = 3 if causal else 1\n",
    "\n",
    "        M = torch.empty((Bs, NH, S), device=q.device, dtype=torch.float32)\n",
    "        if supports_host_descriptor() and not (is_hopper() and warp_specialize):\n",
    "            y_dim = Bs * NH * S\n",
    "            dummy_shape = [1, 1]\n",
    "            q_desc = tl.make_tensor_descriptor(q, shape=[y_dim, HEAD_DIM], strides=[y_dim, 1], block_shape=dummy_shape)\n",
    "            k_desc = tl.make_tensor_descriptor(k, shape=[y_dim, HEAD_DIM], strides=[y_dim, 1], block_shape=dummy_shape)\n",
    "            o_desc = tl.make_tensor_descriptor(o, shape=[y_dim, HEAD_DIM], strides=[y_dim, 1], block_shape=dummy_shape)\n",
    "            if q.dtype == torch.float8_e5m2:\n",
    "                v_desc = tl.make_tensor_descriptor(v, shape=[HEAD_DIM, y_dim], strides=[S, 1], block_shape=dummy_shape)\n",
    "            else:\n",
    "                v_desc = tl.make_tensor_descriptor(v, shape=[y_dim, HEAD_DIM], strides=[y_dim, 1], block_shape=dummy_shape)\n",
    "        else:\n",
    "            q_desc = q\n",
    "            k_desc = k\n",
    "            v_desc = v\n",
    "            o_desc = o\n",
    "\n",
    "        triton.set_allocator(lambda size, align, _: torch.empty(size, dtype=torch.int8, device=\"cuda\"))\n",
    "        ctx.grid = lambda META: (triton.cdiv(S, META[\"BLOCK_M\"]), Bs * NH, 1)\n",
    "        _attn_fwd_kernel[ctx.grid](\n",
    "            sm_scale,\n",
    "            M,\n",
    "            Bs,\n",
    "            NH,\n",
    "            q_desc,\n",
    "            k_desc,\n",
    "            v_desc,\n",
    "            o_desc,\n",
    "            S=S,\n",
    "            HEAD_DIM=HEAD_DIM,\n",
    "            FP8_OUTPUT=q.dtype == torch.float8_e5m2,\n",
    "            STAGE=stage,\n",
    "            warp_specialize=warp_specialize,  #\n",
    "            IS_HOPPER=is_hopper(),  #\n",
    "            **extra_kern_args\n",
    "        )\n",
    "        ctx.save_for_backward(q, k, v, o, M)\n",
    "        ctx.sm_scale = sm_scale\n",
    "        ctx.HEAD_DIM = HEAD_DIM\n",
    "        ctx.causal = causal\n",
    "        return o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
