{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d48089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# Host side TMA api\n",
    "from triton.tools.tensor_descriptor import TensorDescriptor\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _schedule_pid_mn(pid, group_size, m_group_size, m):\n",
    "    group_id = pid // group_size\n",
    "    sub_pid = pid % group_size\n",
    "    m_start = group_id * m_group_size\n",
    "    group_m_size = min(m_group_size, m - m_start)\n",
    "    return m_start + (sub_pid % group_m_size), sub_pid // group_m_size\n",
    "\n",
    "\n",
    "def _get_mm_persistent_tma_config(pre_hook=None):\n",
    "    return [\n",
    "        triton.Config(\n",
    "            {\"BM\": BM, \"BN\": BN, \"BK\": BK, \"GROUP_SIZE_M\": 8, \"EPILOGUE_SUBTILE\": epilogue_subtile},\n",
    "            num_stages=stage,\n",
    "            num_warps=num_warps,\n",
    "            pre_hook=pre_hook,\n",
    "        )\n",
    "        for BM in [128]\n",
    "        for BN in [128, 256]\n",
    "        for BK in [64, 128]\n",
    "        for stage in [2, 3, 4]\n",
    "        for num_warps in [4, 8]\n",
    "        for epilogue_subtile in {True, False}\n",
    "    ]\n",
    "\n",
    "\n",
    "def _set_mma_args_hook(nargs):\n",
    "    BM, BK, BN = nargs[\"BM\"], nargs[\"BK\"], nargs[\"BN\"]\n",
    "    nargs[\"a_desc\"].block_shape = [BM, BK]\n",
    "    nargs[\"b_desc\"].block_shape = [BN, BK]\n",
    "    if nargs[\"EPILOGUE_SUBTILE\"]:\n",
    "        nargs[\"c_desc\"].block_shape = [BM, BN // 2]\n",
    "    else:\n",
    "        nargs[\"c_desc\"].block_shape = [BM, BN]\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=_get_mm_persistent_tma_config(pre_hook=_set_mma_args_hook),\n",
    "    key=[\"M\", \"N\", \"K\"],\n",
    ")\n",
    "@triton.jit\n",
    "def _matmul_tma_persistent_kernel(\n",
    "    a_desc,\n",
    "    b_desc,\n",
    "    c_desc,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    BM: tl.constexpr,\n",
    "    BN: tl.constexpr,\n",
    "    BK: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr,\n",
    "    EPILOGUE_SUBTILE: tl.constexpr,\n",
    "    FP8_OUTPUT: tl.constexpr,\n",
    "    NUM_SMs: tl.constexpr,\n",
    "    WARP_SPECIALIZE: tl.constexpr,\n",
    "):\n",
    "    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16\n",
    "\n",
    "    pid = tl.program_id(axis=0)\n",
    "    m_tiles = tl.cdiv(M, BM)\n",
    "    n_tiles = tl.cdiv(N, BN)\n",
    "    k_tiles = tl.cdiv(K, BK)\n",
    "    total_tiles = m_tiles * n_tiles\n",
    "    group_tiles = GROUP_SIZE_M * n_tiles\n",
    "\n",
    "    tile_c = pid - NUM_SMs\n",
    "    for tile_id in tl.range(pid, total_tiles, NUM_SMs, flatten=True, warp_specialize=WARP_SPECIALIZE):\n",
    "        # reschedule the tile_id to the pid_m and pid_n\n",
    "        pid_m, pid_n = _schedule_pid_mn(tile_id, group_tiles, GROUP_SIZE_M, m_tiles)\n",
    "\n",
    "        off_am = pid_m * BM\n",
    "        off_bn = pid_n * BN\n",
    "        accu = tl.zeros((BM, BN), dtype=tl.float32)\n",
    "        for ki in tl.range(k_tiles):\n",
    "            a = a_desc.load([off_am, ki * BK])\n",
    "            b = b_desc.load([off_bn, ki * BK])\n",
    "            accu = tl.dot(a, b.T, accu)\n",
    "\n",
    "        # actually the same value as tile_id, maually use another variable to promote pipelining\n",
    "        tile_c += NUM_SMs\n",
    "        pid_m, pid_n = _schedule_pid_mn(tile_c, group_tiles, GROUP_SIZE_M, m_tiles)\n",
    "        off_am_c = pid_m * BM\n",
    "        off_bn_c = pid_n * BN\n",
    "\n",
    "        # split the epilogue may reduce the shared memory consumption, thus promote more stages\n",
    "        if EPILOGUE_SUBTILE:\n",
    "            accu = tl.reshape(accu, (BM, 2, BN // 2))\n",
    "            accu = tl.permute(accu, (0, 2, 1))\n",
    "            acc0, acc1 = tl.split(accu)\n",
    "            c0 = acc0.to(dtype)\n",
    "            c_desc.store([off_am_c, off_bn_c], c0)\n",
    "            c1 = acc1.to(dtype)\n",
    "            c_desc.store([off_am_c, off_bn_c + BN // 2], c1)\n",
    "        else:\n",
    "            accu = accu.to(dtype)\n",
    "            c_desc.store([off_am_c, off_bn_c], accu)\n",
    "\n",
    "\n",
    "def my_matmul_tma_persistent(a: torch.Tensor, b: torch.Tensor, warp_specialize=False) -> torch.Tensor:\n",
    "    assert a.shape[1] == b.shape[1]\n",
    "    M, K = a.shape\n",
    "    N, K = b.shape\n",
    "\n",
    "    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n",
    "\n",
    "    dummy_block = [1, 1]\n",
    "    a_desc = TensorDescriptor(a, a.shape, a.stride(), dummy_block)\n",
    "    b_desc = TensorDescriptor(b, b.shape, b.stride(), dummy_block)\n",
    "    c_desc = TensorDescriptor(c, c.shape, c.stride(), dummy_block)\n",
    "\n",
    "    NUM_SMs = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n",
    "\n",
    "    def grid(META):\n",
    "        BM, BN = META[\"BM\"], META[\"BN\"]\n",
    "        return (min(NUM_SMs, triton.cdiv(M, BM) * triton.cdiv(N, BN)),)\n",
    "\n",
    "    _matmul_tma_persistent_kernel[grid](\n",
    "        a_desc, b_desc, c_desc, M, N, K, NUM_SMs=NUM_SMs, FP8_OUTPUT=a.dtype == torch.float8_e4m3fn, WARP_SPECIALIZE=warp_specialize\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477f1cd7",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b37a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  1  2  3  4  5  6 \n",
      " 7  8  9 10 11 12 13 \n",
      "14 15 16 17 18 19 20 \n",
      "21 22 23 24 25 26 27 \n",
      "28 29 30 31 32 33 34 \n",
      "35 36 37 38 39 40 41 \n",
      "42 43 44 45 46 47 48 \n",
      "49 50 51 52 53 54 55 \n",
      "56 57 58 59 60 61 62 \n",
      "63 64 65 66 67 68 69 \n",
      "70 71 72 73 74 75 76 \n",
      "77 78 79 80 81 82 83 \n",
      "84 85 86 87 88 89 90 \n"
     ]
    }
   ],
   "source": [
    "m_group_size = 4\n",
    "n  = 7\n",
    "group_tiles = m_group_size * n\n",
    "m = 13\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        print(f\"{j+i*n:2d}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  4  8 12 16 20 24 \n",
      " 1  5  9 13 17 21 25 \n",
      " 2  6 10 14 18 22 26 \n",
      " 3  7 11 15 19 23 27 \n",
      "28 32 36 40 44 48 52 \n",
      "29 33 37 41 45 49 53 \n",
      "30 34 38 42 46 50 54 \n",
      "31 35 39 43 47 51 55 \n",
      "56 60 64 68 72 76 80 \n",
      "57 61 65 69 73 77 81 \n",
      "58 62 66 70 74 78 82 \n",
      "59 63 67 71 75 79 83 \n",
      "84 85 86 87 88 89 90 \n"
     ]
    }
   ],
   "source": [
    "points = [[0]*n for _ in range(m)]\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        x,y = _schedule_pid_mn.fn(j+i*n, group_tiles, m_group_size, m)\n",
    "        points[x][y] = j+i*n\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        print(f\"{points[i][j]:2d}\", end=\" \")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
