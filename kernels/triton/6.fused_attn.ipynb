{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307d3208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.tools.tensor_descriptor import TensorDescriptor\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "def is_cuda():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
    "\n",
    "def supports_host_descriptor():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9\n",
    "\n",
    "def is_blackwell():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] == 10\n",
    "\n",
    "def is_hopper():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] == 9\n",
    "\n",
    "def get_num_stages_options():\n",
    "    if is_hip():\n",
    "        return [1]\n",
    "    elif supports_host_descriptor():\n",
    "        return [2, 3, 4]\n",
    "    else:\n",
    "        return [2, 3, 4]\n",
    "\n",
    "def _host_descriptor_pre_hook(nargs):\n",
    "    BLOCK_M = nargs[\"BLOCK_M\"]\n",
    "    BLOCK_N = nargs[\"BLOCK_N\"]\n",
    "    HEAD_DIM = nargs[\"HEAD_DIM\"]\n",
    "    if not isinstance(nargs[\"desc_q\"], TensorDescriptor):\n",
    "        return\n",
    "    nargs[\"desc_q\"].block_shape = [BLOCK_M, HEAD_DIM]\n",
    "    if nargs[\"FP8_OUTPUT\"]:\n",
    "        nargs[\"desc_v\"].block_shape = [HEAD_DIM, BLOCK_N]\n",
    "    else:\n",
    "        nargs[\"desc_v\"].block_shape = [BLOCK_N, HEAD_DIM]\n",
    "    nargs[\"desc_k\"].block_shape = [BLOCK_N, HEAD_DIM]\n",
    "    nargs[\"desc_o\"].block_shape = [BLOCK_M, HEAD_DIM]\n",
    "    \n",
    "configs = [\n",
    "    triton.Config({'BLOCK_M': BM, 'BLOCK_N': BN}, num_stages=s, num_warps=w, pre_hook=_host_descriptor_pre_hook) \\\n",
    "    for BM in [64, 128]\\\n",
    "    for BN in [32, 64, 128]\\\n",
    "    for s in get_num_stages_options() \\\n",
    "    for w in [4, 8]\\\n",
    "]\n",
    "\n",
    "def keep(conf):\n",
    "    BLOCK_M = conf.kwargs[\"BLOCK_M\"]\n",
    "    BLOCK_N = conf.kwargs[\"BLOCK_N\"]\n",
    "    if BLOCK_M % BLOCK_N != 0:\n",
    "        return False\n",
    "    return not (is_cuda() and torch.cuda.get_device_capability()[0] == 9 and BLOCK_M * BLOCK_N < 128 * 128\n",
    "                and conf.num_warps == 8)\n",
    "    \n",
    "def prune_invalid_configs(configs, named_args, **kwargs):\n",
    "    S = kwargs[\"S\"]\n",
    "\n",
    "    # Filter out configs where BLOCK_M > N_CTX\n",
    "    return [conf for conf in configs if conf.kwargs.get(\"BLOCK_M\", 0) <= S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ac78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_fwd_inner(acc, l_i, m_i, q,  #\n",
    "                    desc_k, desc_v,  #\n",
    "                    offset_y, dtype: tl.constexpr, start_m, qk_scale,  #\n",
    "                    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  #\n",
    "                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,  #\n",
    "                    N_CTX: tl.constexpr, warp_specialize: tl.constexpr, IS_HOPPER: tl.constexpr):\n",
    "    if STAGE == 1:\n",
    "        # causal stage 1\n",
    "        lo, hi = 0, start_m * BLOCK_M # all preceed tokens is another blockN\n",
    "    elif STAGE == 2:\n",
    "        # causal stage 2\n",
    "        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n",
    "        lo = tl.multiple_of(lo, BLOCK_M)\n",
    "    else:\n",
    "        # causal false\n",
    "        lo, hi = 0, N_CTX\n",
    "\n",
    "    offset_ky = offset_y + lo\n",
    "    if dtype == tl.float8e5:\n",
    "        offset_vy =  HEAD_DIM * offset_y + lo\n",
    "    else:\n",
    "        offset_vy = offset_y + lo\n",
    "\n",
    "    # loop over k, v and update accumulator\n",
    "    for start_n in range(lo, hi, BLOCK_N, warp_specialize=WARP_SPECIALIZE):\n",
    "        start_n = tl.multiple_of(start_n, BLOCK_N)\n",
    "        k = desc_k.load([offset_ky,0]).T\n",
    "        qk = tl.dot(q,k)\n",
    "        if STAGE == 2:\n",
    "            mask = offs_m[:, None] >= (start_n+offs_n[None,:])\n",
    "            qk = qk * qk_scale + tl.where(mask, 0, -1.0e6)\n",
    "            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n",
    "            qk -= m_ij[:,None]\n",
    "        else:\n",
    "            qk = qk * qk_scale\n",
    "            m_ij = tl.maximum(m_i, tl.max(qk, axis=1))\n",
    "            qk -= m_ij[:, None]\n",
    "        \n",
    "        p = tl.math.exp2(qk)\n",
    "        # -- compute correction factor\n",
    "        alpha = tl.math.exp2(m_i - m_ij)\n",
    "        l_ij = tl.sum(p, 1)\n",
    "        \n",
    "        acc = acc * alpha[:, None]\n",
    "        # prepare p and v for the dot\n",
    "        if dtype == tl.float8e5:\n",
    "            v = desc_v.load([0, offsetv_y]).T\n",
    "        else:\n",
    "            v = desc_v.load([offsetv_y, 0])\n",
    "        p = p.to(dtype)\n",
    "        # note that this non transposed v for FP8 is only supported on Blackwell\n",
    "        acc = tl.dot(p, v, acc)\n",
    "        # update m_i and l_i\n",
    "        # place this at the end of the loop to reduce register pressure\n",
    "        l_i = l_i * alpha + l_ij\n",
    "        m_i = m_ij\n",
    "        offsetk_y += BLOCK_N\n",
    "        offsetv_y += BLOCK_N\n",
    "    return acc, l_i, m_i\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _maybe_make_tensor_desc(desc_or_ptr, shape, strides, block_shape):\n",
    "    if isinstance(desc_or_ptr, tl.tensor_descriptor):\n",
    "        return desc_or_ptr\n",
    "    else:\n",
    "        return tl.make_tensor_descriptor(desc_or_ptr, shape, strides, block_shape)\n",
    "\n",
    "\n",
    "@triton.autotune(configs=list(filter(keep, configs)), key=[\"S\", \"HEAD_DIM\", \"FP8_OUTPUT\", \"warp_specialize\"],\n",
    "                 prune_configs_by={'early_config_prune': prune_invalid_configs})\n",
    "@triton.jit\n",
    "def _attn_fwd_kernel(\n",
    "    sm_scale,\n",
    "    M,\n",
    "    Bs,\n",
    "    NH,\n",
    "    q_desc,\n",
    "    k_desc,\n",
    "    v_desc,\n",
    "    o_desc,\n",
    "    S,\n",
    "    HEAD_DIM:tl.constexpr,\n",
    "    BLOCK_M:tl.constexpr,\n",
    "    BLOCK_N:tl.constexpr,\n",
    "    FP8_OUTPUT:tl.constexpr,\n",
    "    STAGE:tl.constexpr,\n",
    "    warp_specialize:tl.constexpr,\n",
    "    IS_HOPPER:tl.constexpr,\n",
    "):\n",
    "    dtype = tl.float8e5 if FP8_OUTPUT else tl.float16\n",
    "    tl.static_assert(BLOCK_N <= HEAD_DIM)\n",
    "    \n",
    "    start_m = tl.program_id(axis=0)\n",
    "    offset_bsh = tl.program_id(axis=1)\n",
    "    offset_bs = offset_bsh // NH\n",
    "    offset_head = offset_bsh % NH\n",
    "    \n",
    "    y_dim = Bs * NH * S\n",
    "    q_desc = _maybe_make_tensor_desc(q_desc, shape=[y_dim, HEAD_DIM], strides=[y_dim, 1], block_shape=[BLOCK_M, HEAD_DIM])\n",
    "    k_desc = _maybe_make_tensor_desc(k_desc, shape=[y_dim, HEAD_DIM], strides=[y_dim, 1], block_shape=[BLOCK_N, HEAD_DIM])\n",
    "    o_desc = _maybe_make_tensor_desc(o_desc, shape=[y_dim, HEAD_DIM], strides=[y_dim, 1], block_shape=[BLOCK_M, HEAD_DIM])\n",
    "    if FP8_OUTPUT:\n",
    "        v_desc = _maybe_make_tensor_desc(v_desc, shape=[HEAD_DIM, y_dim], strides=[S, 1], block_shape=[HEAD_DIM, BLOCK_N])\n",
    "    else:\n",
    "        v_desc = _maybe_make_tensor_desc(v_desc, shape=[y_dim, HEAD_DIM], strides=[y_dim, 1], block_shape=[BLOCK_N, HEAD_DIM])\n",
    "    \n",
    "    offset_y = offset_bsh * S\n",
    "    qo_offset_y = offset_y + start_m * BLOCK_M\n",
    "    \n",
    "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = tl.arange(0, BLOCK_N)\n",
    "    \n",
    "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n",
    "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n",
    "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    qk_scale = sm_scale\n",
    "    qk_scale *= 1.44269504  # 1/log(2)\n",
    "    \n",
    "    q = q_desc.load([qo_offset_y, 0])\n",
    "    \n",
    "    if STAGE & 1:\n",
    "        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q,  #\n",
    "                                        desc_k, desc_v,  #\n",
    "                                        offset_y, dtype, start_m, qk_scale,  #\n",
    "                                        BLOCK_M, HEAD_DIM, BLOCK_N,  #\n",
    "                                        4 - STAGE, offs_m, offs_n, N_CTX,  #\n",
    "                                        warp_specialize, IS_HOPPER)\n",
    "    # stage 2: on-band\n",
    "    if STAGE & 2:\n",
    "        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q,  #\n",
    "                                        desc_k, desc_v,  #\n",
    "                                        offset_y, dtype, start_m, qk_scale,  #\n",
    "                                        BLOCK_M, HEAD_DIM, BLOCK_N,  #\n",
    "                                        2, offs_m, offs_n, N_CTX,  #\n",
    "                                        warp_specialize, IS_HOPPER)\n",
    "        \n",
    "    # epilogue\n",
    "    acc = acc / l_i[:, None]\n",
    "    desc_o.store([qo_offset_y, 0], acc.to(dtype))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d50a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, causal: bool, sm_scale: float, warp_specialize: bool = False):\n",
    "    \"\"\"\n",
    "    q.shape: (Bs, NHead, SeqLen, HeadDim)\n",
    "    \"\"\"\n",
    "    assert q.dim() == 4\n",
    "    Bs, NH, S, HEAD_DIM = q.shape\n",
    "    HEAD_DIM_K, HEAD_DIM_V = k.shape[-1], v.shape[-1]\n",
    "    assert HEAD_DIM == HEAD_DIM_K == HEAD_DIM_V\n",
    "    assert HEAD_DIM_K in {16, 32, 64, 128, 256}\n",
    "\n",
    "    o = torch.empty_like(q)\n",
    "    stage = 3 if causal else 1\n",
    "\n",
    "    M = torch.empty((Bs, NH, S), device=q.device, dtype=torch.float32)\n",
    "    if supports_host_descriptor() and not (is_hopper() and warp_specialize):\n",
    "        y_dim = Bs * NH * S\n",
    "        dummy_shape = [1, 1]\n",
    "        q_desc = TensorDescriptor(q, shape=[y_dim, HEAD_DIM], strides=[y_dim, 1], block_shape=dummy_shape)\n",
    "        k_desc = TensorDescriptor(k, shape=[y_dim, HEAD_DIM], strides=[y_dim, 1], block_shape=dummy_shape)\n",
    "        o_desc = TensorDescriptor(o, shape=[y_dim, HEAD_DIM], strides=[y_dim, 1], block_shape=dummy_shape)\n",
    "        if q.dtype == torch.float8_e5m2:\n",
    "            v_desc = TensorDescriptor(v, shape=[HEAD_DIM, y_dim], strides=[S, 1], block_shape=dummy_shape)\n",
    "        else:\n",
    "            v_desc = TensorDescriptor(v, shape=[y_dim, HEAD_DIM], strides=[y_dim, 1], block_shape=dummy_shape)\n",
    "    else:\n",
    "        q_desc = q\n",
    "        k_desc = k\n",
    "        v_desc = v\n",
    "        o_desc = o\n",
    "\n",
    "    def alloc_fn(size: int, align: int, _):\n",
    "        return torch.empty(size, dtype=torch.int8, device=\"cuda\")\n",
    "    triton.set_allocator(alloc_fn)\n",
    "    \n",
    "    grid = lambda META: (triton.cdiv(S, META[\"BLOCK_M\"]), Bs * NH, 1)\n",
    "    _attn_fwd_kernel[grid](\n",
    "        sm_scale,\n",
    "        M,\n",
    "        Bs,\n",
    "        NH,\n",
    "        q_desc,\n",
    "        k_desc,\n",
    "        v_desc,\n",
    "        o_desc,\n",
    "        S=S,\n",
    "        HEAD_DIM=HEAD_DIM,\n",
    "        FP8_OUTPUT=q.dtype == torch.float8_e5m2,\n",
    "        STAGE=stage,\n",
    "        warp_specialize=warp_specialize,  #\n",
    "        IS_HOPPER=is_hopper(),  #\n",
    "    )\n",
    "    return o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
